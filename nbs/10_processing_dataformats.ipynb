{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/logos.jpg \"MiCMOR, KIT Campus Alpin\")\n",
    "\n",
    "**[MiCMOR](https://micmor.kit.edu) [SummerSchool \"Environmental Data Science: From Data Exploration to Deep Learning\"](https://micmor.kit.edu/sites/default/files/MICMoR%20Summer%20School%202019%20Flyer.pdf)**  \n",
    "IMK-IFU KIT Campus Alpin, Sept. 4 - 13 2019, Garmisch-Partenkirchen, Germany.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data formats \n",
    "\n",
    "... an overview of formats that you might come across and when you might use them. \n",
    "\n",
    "## Text-based formats\n",
    "### Tabular text files: csv, tsv, txt\n",
    "\n",
    "**What to use:** `pandas` (`pd.read_csv()`)\n",
    "\n",
    "I'd say, in general the most common file format you might encounter. Also, potentially messy since there is no true schema to it (there are actually many). Luckily, pandas has a pretty great `read_csv()` Function that can handle pretty much any variant you might come across (it's actually so versatile that it has about 250 arguments you can specify!!!).\n",
    "\n",
    "We cannot possbibly cover all aspects, but some of the more common arguments (at least for me) are:\n",
    "- specifying the seperator (`sep=`)\n",
    "- split at any kind of whitespace (`delim_whitespace=True`)\n",
    "- specifying if there's no header (`header=None`)\n",
    "- giving column names (`names=[\"Col1\",\"Col2\",...]`; if the file has no colnames yet don't forget to specify `header=None`, too)\n",
    "- use a columns to set the index (`index_col='myIndexCol'`)\n",
    "- specify which value should be considered missing/ NaN (`na_values=[-9999, 'na', 'none']`) - this can also be done per-column if you specify a dictionary for your columns with the individual NaN-values to filter\n",
    "- skipping rows (`skiprows=5`)\n",
    "- convert date columns to an actual date type during import (`parse_dates=True` or `parse_dates=['DateCol1', 'DateCol2']`) - you can also specify a custom data/time parser function if pandas has trouble detecting the correct format\n",
    "\n",
    "**Tip:** Since formats are so variable it's often a good idea to peak into the first 10 to 100 lines of a file. A fast and easy way to do this in Jupyter is to issue a bash command inline like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 10 data/testfile.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('data/testfile.csv', index_col=['ID'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML\n",
    "\n",
    "**What to use:** `cElementTree` (or: `lxml`)\n",
    "\n",
    "The Extensible Markup Language (XML) usually encodes a document or data structure. It is organized tree-like thus supports a hierarchy. Usually a schema is defined that defines what a given XML document can contain (like a database schema). The basic building blocks to encapsule data are elements and attributes. You can have elements that enclose text (`<myelement>text</myelement>`) or elements without text (`<myelement />`). Often, you also have attributes in an element (`<myelement attribute1=\"A\" another_attr=\"2\"/>`).\n",
    "\n",
    "A nested XML structure might look like this:\n",
    "\n",
    "```\n",
    "<person sex=\"female\">\n",
    "  <firstname>Anna</firstname>\n",
    "  <lastname>Smith</lastname>\n",
    "</person>\n",
    "```\n",
    "As a small example with data we will use in the Deep Learning lessons. Let's read one of the image description XML files that exist for each image of the ImageCLEF 2013 plant classification challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data/imageclef_2013_sample/3.xml')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "# get xml files in dir and return first one from generator using next()\n",
    "file_path = next(Path('../data/imageclef_2013_sample').glob('*.xml'))\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if we do not have a clear picture what is actually in the file it's a good idea to print it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<Image>\n",
      "  <FileName>3.jpg</FileName>\n",
      "  <IndividualPlantId>742</IndividualPlantId>\n",
      "  <Date>07/07/10</Date>\n",
      "  <Locality>France - Montpellier</Locality>\n",
      "  <GPSLocality>\n",
      "    <Longitude>3.876716</Longitude>\n",
      "    <Latitude>43.610769</Latitude>\n",
      "  </GPSLocality>\n",
      "  <Author>Jean-Francois Molino</Author>\n",
      "  <Organization>Tela Botanica</Organization>\n",
      "  <Type>SheetAsBackground</Type>\n",
      "  <Content>Leaf</Content>\n",
      "  <ClassId>Rhamnus alaternus</ClassId>\n",
      "  <Taxon>\n",
      "    <Regnum>Plantae</Regnum>\n",
      "    <Class>Equisetopsida C. Agardh</Class>\n",
      "    <Subclass>Magnoliidae NovÃ¡k ex Takht.</Subclass>\n",
      "    <Superorder>Rosanae Takht.</Superorder>\n",
      "    <Order>Rosales Bercht. &amp; J. Presl</Order>\n",
      "    <Family>Rhamnaceae Juss.</Family>\n",
      "    <Genus>Rhamnus L.</Genus>\n",
      "    <Species>Rhamnus alaternus L.</Species>\n",
      "  </Taxon>\n",
      "  <VernacularNames>Italian buckthorn</VernacularNames>\n",
      "  <Year>ImageCLEF2011</Year>\n",
      "  <IndividualPlantId2012>201</IndividualPlantId2012>\n",
      "  <ImageID2012>4411.jpg</ImageID2012>\n",
      "</Image>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that we can use python variables in a bash command with the curly brackets !!!\n",
    "! cat {file_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this schema apparently does not use any attributes, just elements..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element 'Image' at 0x7f3bed16f228>\n"
     ]
    }
   ],
   "source": [
    "# parse the entire file\n",
    "xml = ET.parse(file_path)\n",
    "\n",
    "# get the document root element\n",
    "root = xml.getroot()\n",
    "\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot do a full XML tutorial here, but you can either iterate through elements or you can search for a specific one.\n",
    "The following code searches for the first occurance of the subelement Family and return the text enclosed by the element\n",
    "(if it had an attribute we could query it using `.attrib['attributeName']`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rhamnaceae Juss.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.find('./Taxon/Family').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Note:** \n",
    "- Using the package `xmltodict` (not shown) we can also read in a XML file and convert it to a nested python dictionary. We then could pass it on and convert it into a JSON structure (which basically is a nested dictionary).\n",
    "- If our file had many entries of *Image*, one would iterate over them like this:\n",
    "\n",
    "```python\n",
    "for child in root:\n",
    "    print(child.tag, child.attrib, child.text)\n",
    "```\n",
    "- More on using ElementTree [here](https://docs.python.org/3/library/xml.etree.elementtree.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "JavaScript Object Notation (JSON) was inspired by a subset of the JavaScript programming language dealing with object literal syntax but nowadays has its own standard. JSON supports primitive types, like strings and numbers, as well as nested lists and objects.\n",
    "\n",
    "An example looks like this:\n",
    "```JSON\n",
    "{\n",
    "    \"firstName\": \"Jane\",\n",
    "    \"lastName\": \"Doe\",\n",
    "    \"hobbies\": [\"running\", \"sky diving\", \"singing\"],\n",
    "    \"age\": 35,\n",
    "    \"children\": [\n",
    "        {\n",
    "            \"firstName\": \"Alice\",\n",
    "            \"age\": 6\n",
    "        },\n",
    "        {\n",
    "            \"firstName\": \"Bob\",\n",
    "            \"age\": 8\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read in a JSON file like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_file.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-34e4acbf617f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# reading a json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_file.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_file.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# reading a json file\n",
    "with open(\"data_file.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "\n",
    "# dumping a json file\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most often, you'd probably pull JSON files from the web. Here is a small example how you do this with `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error_message': 'You must use an API key to authenticate each request to Google Maps Platform APIs. For additional information, please refer to http://g.co/dev/maps-no-account', 'results': [], 'status': 'REQUEST_DENIED'}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json \n",
    "with urllib.request.urlopen(\"http://maps.googleapis.com/maps/api/geocode/json?address=google\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this request actually yields an error since we did not specify an API token to use this google API(which is encoded in JSON as well). Moving on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have a JSON structure in a string (because you fulled it in from the web etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'researcher': {'name': 'Ford Prefect',\n",
       "  'species': 'Betelgeusian',\n",
       "  'relatives': [{'name': 'Zaphod Beeblebrox', 'species': 'Betelgeusian'}]}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_string = \"\"\"\n",
    "{\n",
    "    \"researcher\": {\n",
    "        \"name\": \"Ford Prefect\",\n",
    "        \"species\": \"Betelgeusian\",\n",
    "        \"relatives\": [\n",
    "            {\n",
    "                \"name\": \"Zaphod Beeblebrox\",\n",
    "                \"species\": \"Betelgeusian\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "data = json.loads(json_string)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary formats\n",
    "### Excel ðŸ¤¨\n",
    "\n",
    "Ok, **you really should be using csv files instead**. However, if you need to import Excel files pandas can do it (if you also install `xlrd`)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_excel(open('../data/other/NO2_2017.xlsx', 'rb')) #, sheet_name='Sheet1')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pkl\n",
    "### netcdf3/4\n",
    "### sql\n",
    "### hdf5/ h5\n",
    "### npy/ npz\n",
    "## Geographic data\n",
    "### Shapefiles\n",
    "### Geotiffs\n",
    "## Other\n",
    "### parquet\n",
    "### feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
